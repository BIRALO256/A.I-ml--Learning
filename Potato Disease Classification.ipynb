{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "a7550d5e-65c1-4216-8bdf-00dba14fb63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2152 files belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "# creating a pipline for my dataset  using TensorFlow's image_dataset_from_directory function\n",
    "dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    \"PlantVillage\",\n",
    "    shuffle =True, \n",
    "    image_size = (IMAGE_SIZE,IMAGE_SIZE),\n",
    "    batch_size =BATCH_SIZE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2bea9c72-f9b9-4a39-ac08-3d1353bcd0b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Potato___Early_blight', 'Potato___Late_blight', 'Potato___healthy']"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## btw the labels for the images are based on the folder names , that's how images are categorized .. so basically the folder names are the class names\n",
    "class_names = dataset.class_names\n",
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7d5afcd9-f1ab-4cb7-acfc-788000399a5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.400000000000006"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the number of training samples (80% of the dataset)\n",
    "train_size = 0.8\n",
    "len(dataset) * train_size  # Number of samples for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "b36aad2c-3f8f-4ba2-b13c-277d5e6d1cda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Skip the first 54 batches (already used for training) and use the remaining data for testing and validation\n",
    "test_ds = dataset.skip(54)\n",
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c1fe5e78-6dbf-4004-ad9c-702c3fd37cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.800000000000001"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the number of samples for the validation set (10% of the total dataset)\n",
    "val_size = 0.1\n",
    "len(dataset) * val_size  # Number of samples for validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b18c9135-fd59-4598-9550-d6f16f158c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## for our test data set \n",
    "test_ds = test_ds.skip(6)\n",
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "351422fa-b62a-4937-bf52-3d054b06d594",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_partitions_tf(ds, train_split =0.8, val_split =0.1, test_split =0.1, shuffle =True,  shuffle_size =10000):\n",
    "    # Get the total size of the dataset\n",
    "    ds_size =len(ds)\n",
    "     # Shuffle the dataset if the 'shuffle' parameter is set to True\n",
    "    if shuffle:\n",
    "        ds =ds.shuffle(shuffle_size, seed =12) # Randomize the dataset with a buffer size\n",
    "        \n",
    "    # Calculate the number of samples for each split (train, validation, and test)\n",
    "    train_size = int(train_split *ds_size) # Number of samples for training\n",
    "    val_size = int(val_split * ds_size) # Number of samples for validation\n",
    "\n",
    "    # Create the training dataset by taking the first 'train_size' samples\n",
    "    train_ds =ds.take(train_size)\n",
    "    # Create the validation dataset by skipping 'train_size' samples and taking 'val_size' samples\n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "     # Create the test dataset by skipping both 'train_size' and 'val_size' samples\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "    # Return the partitioned datasets\n",
    "    return train_ds, val_ds,test_ds\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e687de6f-5301-4cd6-becb-d9dce661e411",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the 'get_dataset_partitions_tf' function to split the dataset into training, validation, and test sets\n",
    "train_ds, val_ds, test_ds = get_dataset_partitions_tf(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "feb6c77d-4bc1-4e1b-b8a9-d4bdc54c4497",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "1b1d0153-382a-41f3-adeb-0d769b973d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "6b8d4ccf-2daf-4cfe-a854-df0b481c4bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "536b2d49-6027-4f97-a90e-aa22f7b9e5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache the dataset to improve performance by storing data in memory, then shuffle it with a buffer of 10,000 items, \n",
    "# and prefetch data to allow parallel loading while training for faster data processing.\n",
    "train_ds = train_ds.cache().shuffle(10000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Same process applied to the validation dataset: caching, shuffling, and prefetching for efficient data pipeline.\n",
    "val_ds = train_ds.cache().shuffle(10000).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Same process applied to the test dataset to ensure efficient loading during evaluation.\n",
    "test_ds = train_ds.cache().shuffle(10000).prefetch(buffer_size=tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d0fffabe-223e-4f5f-9a8f-86b6230aece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequential preprocessing pipeline\n",
    "resize_and_rescale = tf.keras.Sequential([\n",
    "   \n",
    "   # Resize all images to a fixed size (IMAGE_SIZE x IMAGE_SIZE)\n",
    "   tf.keras.layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n",
    "   \n",
    "   # Rescale pixel values from [0, 255] to [0, 1] by dividing by 255\n",
    "   tf.keras.layers.Rescaling(1.0/255)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "210465d5-96e9-444e-b3c2-b0db5a9b095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data argumattion \n",
    "\n",
    "data_augmentation = tf.keras.Sequential([ \n",
    "   # Randomly flip the images horizontally\n",
    "   tf.keras.layers.RandomFlip('horizontal_and_vertical'),\n",
    "   # Randomly rotate images by up to 40 degrees\n",
    "   tf.keras.layers.RandomRotation(0.2),\n",
    "   # Randomly zoom into images by a factor of 0.2\n",
    "   tf.keras.layers.RandomZoom(0.2),\n",
    "   # Randomly change the brightness of the images\n",
    "   tf.keras.layers.RandomBrightness(0.2)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037a5f4f-c699-4b42-8766-ebc768bf0ffe",
   "metadata": {},
   "source": [
    "#BUILD CNN AND TRAIN THIS NETWORK ON THE TRAIN DATASET AFTER WE HAVE DONE LOADING AND SOME PRE-PROCESSING UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0b49db30-736c-41ba-80f4-aa99b4bfd7f1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "input_shape = (BATCH_SIZE,IMAGE_SIZE,IMAGE_SIZE,CHANNELS)\n",
    "n_classes =3\n",
    "\n",
    "model =models.Sequential([\n",
    "\n",
    "    resize_and_rescale , # our first layer will be resizing and rescalling the image \n",
    "    data_augmentation ,# then data augmentation\n",
    "    layers.Conv2D(32,(3,3),activation ='relu',input_shape= input_shape), # third layer is the conv layer\n",
    "    layers.MaxPooling2D((2,2)), ## fouth layer is max pooling to reduce the size of the image but at the same time keeping the most import features available\n",
    "    layers.Conv2D(64, kernel_size=(3,3),activation ='relu'), \n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, kernel_size=(3,3),activation ='relu'), \n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, kernel_size=(3,3),activation ='relu'), \n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, kernel_size=(3,3),activation ='relu'), \n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Flatten(), # then a layer for flattening the out so that it's an  array  of neurons  and then a hidden layer next\n",
    "    layers.Dense(64, activation ='relu') ,# then dense layer\n",
    "    layers.Dense(n_classes, activation ='softmax'), # then my last layer will have three neurons\n",
    "    \n",
    "])\n",
    "\n",
    "model.build(input_shape =input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e75671-6e2d-41ba-8817-7cfc9ddc1650",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
